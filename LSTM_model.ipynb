{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "yr2fNcDrz1To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import library"
      ],
      "metadata": {
        "id": "OmjWHR9K1HXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "metadata": {
        "id": "RlaEgAXcz-zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset"
      ],
      "metadata": {
        "id": "lVL1ckue1Mrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa/SriLanka_Weather_Dataset_updated (1).csv'\n",
        "\n",
        "\n",
        "data_set = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "WiLBdVbg0CpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Columns to drop"
      ],
      "metadata": {
        "id": "yJgYYJID1Q_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "need_drop = [\"temperature_2m_max\",\"temperature_2m_min\", \"sunrise\", \"sunset\", \"apparent_temperature_min\",\n",
        "             \"apparent_temperature_mean\", \"shortwave_radiation_sum\", \"rain_sum\",\n",
        "             \"apparent_temperature_max\",\"snowfall_sum\",\"country\",\"weathercode\"]\n",
        "data_set.drop(columns=need_drop, inplace=True)"
      ],
      "metadata": {
        "id": "4h-gVg4p0Ewm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert date column to datetime data type"
      ],
      "metadata": {
        "id": "30qIzHdw1UpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "data_set['time'] = pd.to_datetime(data_set['time'])\n"
      ],
      "metadata": {
        "id": "OqQl8nLR0GnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose a city"
      ],
      "metadata": {
        "id": "MasNRA9h1Y63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "city_name = \"Jaffna\"\n",
        "data = data_set[data_set[\"city\"] == city_name].drop('city', axis=1)"
      ],
      "metadata": {
        "id": "3pAhv6Zd0IJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set data index as datetime column"
      ],
      "metadata": {
        "id": "2qZhICeu1bfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data.index = pd.to_datetime(data['time'])\n",
        "data = data.drop('time', axis=1)\n"
      ],
      "metadata": {
        "id": "NlBdRsjR0Ki6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill missing values"
      ],
      "metadata": {
        "id": "Y-4V62OL1eYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = data.fillna(method='ffill')"
      ],
      "metadata": {
        "id": "TKsw1qOd0PxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select the target columns"
      ],
      "metadata": {
        "id": "DchDnlFQ1hCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_cols = ['temperature_2m_mean', 'precipitation_sum', 'precipitation_hours', 'et0_fao_evapotranspiration']\n"
      ],
      "metadata": {
        "id": "rZtnETT80Sc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize the data"
      ],
      "metadata": {
        "id": "9UZzPEvP1lEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)"
      ],
      "metadata": {
        "id": "ie4cTcPu0Vn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define sequence length and features"
      ],
      "metadata": {
        "id": "fFkFI89x1n-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sequence_length = 10  # Number of time steps in each sequence\n",
        "num_features = len(data.columns)\n",
        "num_targets = len(target_cols)"
      ],
      "metadata": {
        "id": "QmNUV7Nk0Xn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create sequences and corresponding labels"
      ],
      "metadata": {
        "id": "brbiLg_T1qDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sequences = []\n",
        "labels = []\n",
        "for i in range(len(scaled_data) - sequence_length):\n",
        "    seq = scaled_data[i:i+sequence_length]\n",
        "    label = scaled_data[i+sequence_length][:num_targets]\n",
        "    sequences.append(seq)\n",
        "    labels.append(label)"
      ],
      "metadata": {
        "id": "gceqX1ts0ZnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Convert to numpy arrays"
      ],
      "metadata": {
        "id": "IKNecUN51r63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "sJ3bUaLc0cOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split into train and test sets"
      ],
      "metadata": {
        "id": "JwNByuFU1t7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_size = int(0.8 * len(sequences))\n",
        "train_x, test_x = sequences[:train_size], sequences[train_size:]\n",
        "train_y, test_y = labels[:train_size], labels[train_size:]\n",
        "\n",
        "print(\"Train X shape:\", train_x.shape)\n",
        "print(\"Train Y shape:\", train_y.shape)\n",
        "print(\"Test X shape:\", test_x.shape)\n",
        "print(\"Test Y shape:\", test_y.shape)"
      ],
      "metadata": {
        "id": "NZivf5pa0e7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the LSTM model"
      ],
      "metadata": {
        "id": "1BE5yzhf1vra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add LSTM layers with dropout\n",
        "model.add(LSTM(units=128, input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=32, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Add a dense output layer with multiple outputs\n",
        "model.add(Dense(units=num_targets))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JUeyUHAC0obm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "1YM_CX9m1zJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,  # Use part of the training data as validation\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "Vv3TidI60s1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model on the test set"
      ],
      "metadata": {
        "id": "l8a84dgd13B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_loss = model.evaluate(test_x, test_y)\n",
        "print(\"Test Loss:\", test_loss)"
      ],
      "metadata": {
        "id": "LBdOKsfA0xJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot training & validation loss values"
      ],
      "metadata": {
        "id": "NtZYWbzN14vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LPFkEH-v07dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "jMP2PZbX17F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions = model.predict(test_x)\n"
      ],
      "metadata": {
        "id": "9a19OlCi1Fje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Calculate evaluation metrics for each target variable"
      ],
      "metadata": {
        "id": "k2yqiJg_1-ZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0uaMOdhv34k"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i, col in enumerate(target_cols):\n",
        "    mae = mean_absolute_error(test_y[:, i], predictions[:, i])\n",
        "    mse = mean_squared_error(test_y[:, i], predictions[:, i])\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"\\nMetrics for {col}:\")\n",
        "    print(\"Mean Absolute Error (MAE):\", mae)\n",
        "    print(\"Mean Squared Error (MSE):\", mse)\n",
        "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot actual vs predicted values for each target variable"
      ],
      "metadata": {
        "id": "beeJ61zk2A9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i, col in enumerate(target_cols):\n",
        "    test_y_copies = np.repeat(test_y[:, i].reshape(-1, 1), test_x.shape[-1], axis=-1)\n",
        "    true_values = scaler.inverse_transform(test_y_copies)[:, i]\n",
        "\n",
        "    prediction_copies = np.repeat(predictions[:, i].reshape(-1, 1), test_x.shape[-1], axis=-1)\n",
        "    predicted_values = scaler.inverse_transform(prediction_copies)[:, i]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data.index[-len(true_values):], true_values, label='Actual')\n",
        "    plt.plot(data.index[-len(predicted_values):], predicted_values, label='Predicted')\n",
        "    plt.title(f'{col} Prediction vs Actual')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(col)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "u1gYNEJ-05SF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}